---
permalink: /
title: "Jie Wu Ê≠¶Êù∞"
excerpt: "Jie Wu Ê≠¶Êù∞"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi! I'm Jie Wu, currently a second-year M.S. student in Artificial Intelligence at [SIGS](https://www.sigs.tsinghua.edu.cn/en/), Tsinghua University, where I am fortunate to be advised by Prof. [Yujiu Yang](https://sites.google.com/view/iigroup-thu/about). Prior to this, I received my Bachelor's degree in Computer Science from Wuhan University in June 2024, where I was fortunate to work under the guidance of Prof. [Zheng Wang](https://wangzwhu.github.io/home/) and Assoc. Prof. [Zuchao Li](https://zcli-charlie.github.io/).

Education
======

- **Aug. 2024 - Jun. 2027** M.Sc., SIGS, Tsinghua University, ShenZhen, China.
<br>*GPA: 3.81/4.0*

- **Sep. 2020 - Jun. 2024** B.Sc., School of Computer Science, Wuhan Univeristy, Wuhan, China.
<br>*GPA: 3.98/4.0, Rank: 1/223*


Selected Publications
======
(\* indicates equal contribution)

- [NeurIPS 2024 Oral] [*Rho-1: Not All Tokens Are What You Need*](https://huggingface.co/papers/2404.07965) [[code](https://github.com/microsoft/ToRA)] ([üèÜBest Paper Runner-up Award](https://blog.neurips.cc/2024/12/10/announcing-the-neurips-2024-best-paper-awards/))
<br> Zhenghao Lin\*, **Zhibin Gou**\*, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, Weizhu Chen
<br> Rho-1 introduces Selective Language Modeling (SLM), a method for token-level pretraining data selection. By applying SLM to math continual pretraining, it enhances math reasoning by over 16%, reaching baseline performance 5-10x faster.


<!-- [![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=microsoft&repo=rho)](https://github.com/microsoft/rho) -->

<!-- [![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=microsoft&repo=ToRA)](https://github.com/microsoft/ToRA) -->


<!-- Please see my [Google Scholar profile](https://scholar.google.com/citations?hl=en&user=jTMOma8AAAAJ) for more papers. -->

Experience
======
- (June. 2024 - Present) Researcher, DeepSeek, Beijing, China.
<br> Working on LLMs, especially in reasoning and RL.
<br> Represented work: [DeepSeek-R1](https://arxiv.org/pdf/2501.12948)

- (Jan. 2023 - May. 2024) Research Intern, NLC Group, Microsoft Research Asia, Beijing, China.
<br> Mentor: [Yeyun Gong](https://www.microsoft.com/en-us/research/people/yegong/), [Weizhu Chen](https://www.microsoft.com/en-us/research/people/wzchen/)
<br> Working on LLMs, focusing on reasoning and tool-use.
<br> Represented work: [ToRA](https://microsoft.github.io/ToRA/), [CRITIC](https://arxiv.org/abs/2305.11738), [Rho-1](https://huggingface.co/papers/2404.07965)

- (Sep. 2021 - May. 2022) Research Intern, General Dialogue Group, Baidu Inc., Beijing, China.
<br> Mentor: Xinchao Xu, [Hua Wu](https://wuhuanlp.github.io/)
<br> Working on open-domain dialog: long-term memory, personalized chatbot.


Competitions
======
- (Nov. 2024) **1st** Place (1/150) in LLM merging task of [NeurIPS 2024 LLM Merging Competition](https://www.kaggle.com/competitions/llm-merging-competition/leaderboard)
- (Nov. 2024) **4nd** Place (2/684) in [Kingsoft Office 2024 Algorithm Challenge: Chinese Text Correction Competition](https://datastudio.wps.cn/matchcenter/competition/1/leader-boardÔºâ

Honors & Awards
======
- Top 1% Graduate, Wuhan University, 2024
- Outstanding Graduate Thesis, Wuhan University, 2024
- National Scholarship, Ministry of Education, China, 2023 
