---
permalink: /
title: "Zhibin Gou 苟志斌"
excerpt: "Zhibin Gou 苟志斌"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi! I’m Zhibin Gou, currently a third-year M.S. student at [SIGS](https://www.sigs.tsinghua.edu.cn/en/), Tsinghua University, advised by Prof. [Yujiu Yang](https://sites.google.com/view/iigroup-thu/about). Before that, I received my bachelor's degree in Computer Science from Beijing University of Posts and Telecommunications in Jun. 2022.

My long-term research interest lies in AGI, with a current focus on reasoning and pre-training in LLMs.  I am dedicated to developing simple, general, and scalable pathways to achieve AGI.


Education
======

- **Aug. 2022 - Jun. 2025 (Expected)** M.Sc., Div. of Information Science and Technology, SIGS, Tsinghua University, Beijing, China.
<br>*GPA: 4.0/4.0*

- **Sep. 2018 - Jun. 2022** B.Sc., School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China.
<br>*GPA: 3.84/4.0, Rank: 1%*


Selected Publications
======
(\* indicates equal contribution)

- [ICLR 2024] [*ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving*](https://arxiv.org/abs/2309.17452) [[website](https://microsoft.github.io/ToRA/)][[code](https://github.com/microsoft/ToRA)] (Stars: 900+)
<br> **Zhibin Gou**\*, Zhihong Shao\*, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen
<br> ToRA solves math problems by integrating chain-of-thought reasoning with program-based tool use. ToRA-34B is the first open-source LLM that achieves over 50% on MATH, which is competitive with GPT-4 solving problems with programs.

- [ICLR 2024] [*CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing*](https://arxiv.org/abs/2305.11738) [[code](https://github.com/microsoft/ProphetNet/tree/master/CRITIC)] (Citations: 200+)
<br> **Zhibin Gou**, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen
<br> The first paper to find that current LLMs struggle with Self-Verification and Self-Correction, and propose tool-augmented critiquing for reliable self-improvement.

- [NeurIPS 2024 Oral] [*Rho-1: Not All Tokens Are What You Need*](https://huggingface.co/papers/2404.07965) [[code](https://github.com/microsoft/ToRA)]
<br> Zhenghao Lin\*, **Zhibin Gou**\*, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, Weizhu Chen
<br> Rho-1 introduces Selective Language Modeling (SLM), a method for token-level pretraining data selection. By applying SLM to math continual pretraining, it enhances math reasoning by over 16%, reaching baseline performance 5-10x faster.

-  [EMNLP 2024] [*SciAgent: Tool-augmented Language Models for Scientific Reasoning*](https://arxiv.org/abs/2402.11451)
<br> Yubo Ma, **Zhibin Gou**, Junheng Hao, Ruochen Xu, Shuohang Wang, Liangming Pan, Yujiu Yang, Yixin Cao, Aixin Sun, Hany Awadalla, Weizhu Chen

- [ACL 2024 findings] [*CriticBench: Benchmarking LLMs for Critique-Correct Reasoning*](https://arxiv.org/abs/2402.14809) [[website](https://criticbench.github.io/)] [[code](https://github.com/CriticBench/CriticBench)]
<br> Zicheng Lin\*, **Zhibin Gou**\*, Tian Liang, Ruilin Luo, Haowei Liu, Yujiu Yang

- [COLM 2024] [*Exploring the Mystery of Influential Data for Mathematical Reasoning*](https://arxiv.org/pdf/2404.01067)
<br> Xinzhe Ni, Yeyun Gong, **Zhibin Gou**, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen

- [ACL 2023] [*MvP: Multi-view Prompting Improves Aspect Sentiment Tuple Prediction*](https://arxiv.org/abs/2305.12627) [[code](https://github.com/ZubinGou/multi-view-prompting)]
<br> **Zhibin Gou**\*, Qingyan Guo\*, Yujiu Yang
<br> MvP is a simple unified generative framework for structure prediction, achieving state-of-the-art performance on 10 datasets across 4 ABSA tasks.

- [ACL 2022 findings] [*Long Time No See! Open-Domain Conversation with Long-Term Persona Memory*](https://arxiv.org/abs/2203.05797) [[data](https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2022-DuLeMon)]
<br> Xinchao Xu\*, **Zhibin Gou**\*, Wenquan Wu, Zheng-Yu Niu, Hua Wu, Haifeng Wang, and Shihang Wang
<br> The first long-term memory conversation task and the largest multi-turn mutual persona chat dataset in Chinese.



<!-- [![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=microsoft&repo=rho)](https://github.com/microsoft/rho) -->


<!-- [![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=microsoft&repo=ToRA)](https://github.com/microsoft/ToRA) -->


Preprints
======

- [Arxiv 2024.06] [*DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence*](https://arxiv.org/pdf/2406.11931)
<br> DeepSeek-AI

- [Arxiv 2024.03] [*Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning*](https://arxiv.org/abs/2403.02333)
<br> Yiming Huang, Xiao Liu, Yeyun Gong, **Zhibin Gou**, Yelong Shen, Nan Duan, Weizhu Chen



<!-- Please see my [Google Scholar profile](https://scholar.google.com/citations?hl=en&user=jTMOma8AAAAJ) for more papers. -->

Experience
======
- (Jan. 2023 - May. 2024) Reserch Intern, NLC Group, Microsoft Research Asia, Beijing, China.
<br> Mentor: [Yeyun Gong](https://www.microsoft.com/en-us/research/people/yegong/), [Weizhu Chen](https://www.microsoft.com/en-us/research/people/wzchen/)
<br> Working on large language models, focusing on reasoning and tool-use.

- (Sep. 2021 - May. 2022) Research Intern, General Dialogue Group, Baidu Inc., Beijing, China.
<br> Mentor: Xinchao Xu, [Hua Wu](https://wuhuanlp.github.io/)
<br> Working on open-domain dialog: long-term memory, personalized and safe chatbot.


Competitions
======
- (Nov. 2022) **1st** Place (1/54) in NLP task of [NeurIPS 2022 IGLU Challenge](https://www.aicrowd.com/challenges/neurips-2022-iglu-challenge)
- (Aug. 2022) **2nd** Place (2/684) in [WAIC 2022 Midu CTC Competition](https://aistudio.baidu.com/aistudio/competition/detail/404/0/leaderboard)
- (May. 2021) **1st** Place (auto eval: 1/862, human eval: 3/862) in Multi-skill Dialog task of [NLPCC 2021 Language and Intelligence Challenge](http://tcci.ccf.org.cn/conference/2021/cfpp.php)


Honors & Awards
======
- Outstanding Graduate Thesis, Beijing, 2022
- Outstanding Graduate, Beijing, 2022
- National Scholarship (top 1%), Ministry of Education, China, 2020 
- National Scholarship (top 1%), Ministry of Education, China, 2019 
- National Scholarship (top 1%), Ministry of Education, China, 2018
