---
permalink: /
title: "Jie Wu Ê≠¶Êù∞"
excerpt: "Jie Wu Ê≠¶Êù∞"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi! I'm Jie Wu, currently a second-year M.S. student in Artificial Intelligence at [SIGS](https://www.sigs.tsinghua.edu.cn/en/), Tsinghua University, where I am fortunate to be advised by Prof. [Yujiu Yang](https://sites.google.com/view/iigroup-thu/about). Prior to this, I received my Bachelor's degree in Computer Science from Wuhan University in June 2024, where I was fortunate to work under the guidance of Prof. [Zheng Wang](https://wangzwhu.github.io/home/) and Assoc. Prof. [Zuchao Li](https://zcli-charlie.github.io/).

Education
======

- **Aug. 2024 - Jun. 2027** M.Sc., SIGS, Tsinghua University, ShenZhen, China.
<br>*GPA: 3.81/4.0*

- **Sep. 2020 - Jun. 2024** B.Sc., School of Computer Science, Wuhan Univeristy, Wuhan, China.
<br>*GPA: 3.98/4.0, Rank: 1/223*


Selected Publications
======
(\* indicates equal contribution)

- [NeurIPS 2024 Oral] [*Rho-1: Not All Tokens Are What You Need*](https://huggingface.co/papers/2404.07965) [[code](https://github.com/microsoft/ToRA)] ([üèÜBest Paper Runner-up Award](https://blog.neurips.cc/2024/12/10/announcing-the-neurips-2024-best-paper-awards/))
<br> Zhenghao Lin\*, **Zhibin Gou**\*, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, Weizhu Chen
<br> Rho-1 introduces Selective Language Modeling (SLM), a method for token-level pretraining data selection. By applying SLM to math continual pretraining, it enhances math reasoning by over 16%, reaching baseline performance 5-10x faster.


<!-- [![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=microsoft&repo=rho)](https://github.com/microsoft/rho) -->

<!-- [![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=microsoft&repo=ToRA)](https://github.com/microsoft/ToRA) -->


<!-- Please see my [Google Scholar profile](https://scholar.google.com/citations?hl=en&user=jTMOma8AAAAJ) for more papers. -->

Experience
======
- (Nov. 2024 - Present) Research Intern, Microsoft, Beijing, China.
- <br> Mentor: [Xin Zhang](https://openreview.net/profile?id=~Xin_Zhang42)
<br> Working on post-training for Code LLMs

- (May. 2024 - Sep. 2024) Research Intern, Wuhan University, Wuhan, China.
<br> Mentor: [Zuchao Li]([https://www.microsoft.com/en-us/research/people/yegong/](https://zcli-charlie.github.io/))
<br> Working on enhancing complex reasoning capability of LLMs in cross-document reasoning tasks.

- (May. 2023 - May. 2024) Research Intern, AIM Lab, Wuhan University, Wuhan, China.
<br> Mentor: [Zheng Wang](https://wangzwhu.github.io/home/)
<br> Working on detection for multi-modal misinformation.


Competitions
======
- (Nov. 2024) **1st** Place (1/150) in LLM merging task of [NeurIPS 2024 LLM Merging Competition](https://www.kaggle.com/competitions/llm-merging-competition/leaderboard)
- (Nov. 2024) **4th** Place (4/53) in [Kingsoft Office 2024 Algorithm Challenge: Chinese Text Correction Competition](https://datastudio.wps.cn/matchcenter/competition/1/leader-board)

Honors & Awards
======
- Top 1% Graduate, Wuhan University, 2024
- Outstanding Graduate Thesis, Wuhan University, 2024
- National Scholarship, Ministry of Education, China, 2023 
